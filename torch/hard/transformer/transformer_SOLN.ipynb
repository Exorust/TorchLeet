{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: Build a Transformer Model from Scratch\n",
    "\n",
    "## Objective\n",
    "Implement a **Transformer model** in PyTorch for sequence processing and prediction. The model should include an embedding layer, a Transformer encoder, and an output projection layer.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. Implement Positional Encoding to inject sequence order into embeddings  \n",
    "Create sinusoidal positional encodings that are added to input embeddings to provide order information.\n",
    "\n",
    "2. Implement Multi-Head Self Attention mechanism  \n",
    "Apply attention in parallel across multiple heads to capture different representation subspaces.\n",
    "\n",
    "3. Linear projection of queries, keys, and values  \n",
    "Use a single linear layer to project input into concatenated Q, K, V tensors.\n",
    "\n",
    "4. Scaled dot-product attention  \n",
    "Compute attention scores by scaled dot product of queries and keys, followed by softmax and application to values.\n",
    "\n",
    "5. Output projection after head concatenation  \n",
    "Concatenate the outputs of all heads and project back to the original embedding dimension.\n",
    "\n",
    "6. Implement FeedForward layer used within Transformer blocks  \n",
    "Build a two-layer MLP with a ReLU activation in between to process each token independently.\n",
    "\n",
    "7. Connect components in a TransformerEncoderLayer with proper layer normalization and residual connections  \n",
    "Apply residual connections and layer normalization around the attention and feedforward sublayers.\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Support padded input sequences for variable-length data.\n",
    "- Ensure the model handles batched inputs with correct tensor shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T10:15:05.589280Z",
     "start_time": "2026-01-09T10:15:05.009022Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T11:20:01.772926Z",
     "start_time": "2026-01-09T11:20:01.761229Z"
    }
   },
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # Shape: [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len]\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        qkv = self.qkv_proj(x)  # Shape: (B, T, 3*D)\n",
    "        qkv = qkv.reshape(B, T, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each is (B, num_heads, T, head_dim)\n",
    "\n",
    "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, num_heads, T, T)\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_output = attn_weights @ v  # (B, num_heads, T, head_dim)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(B, T, D)\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(ff_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = FeedForward(embed_dim, ff_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, num_heads, ff_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_proj = nn.Linear(embed_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        # Prepend CLS token to the sequence\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return self.output_proj(x[:, 0])  # Use CLS token for classification\n"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Permutation Parity Test\n",
    "A synthetic task to test the Transformer's ability to recognize sequence patterns. The model should predict whether a sequence of integers has an even or odd number of inversions. Great for testing positional encoding."
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2026-01-09T11:26:13.351790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "torch.manual_seed(42)\n",
    "\n",
    "seq_length = 10\n",
    "num_samples = 5000\n",
    "vocab_size = seq_length  # tokens are 0..N-1\n",
    "\n",
    "def permutation_parity(perm):\n",
    "    inv = 0\n",
    "    for i in range(len(perm)):\n",
    "        for j in range(i + 1, len(perm)):\n",
    "            if perm[i] > perm[j]:\n",
    "                inv += 1\n",
    "    return inv % 2  # 0 = even, 1 = odd\n",
    "\n",
    "X = torch.zeros(num_samples, seq_length, dtype=torch.long)\n",
    "y = torch.zeros(num_samples, dtype=torch.long)\n",
    "X_test = torch.zeros(num_samples, seq_length, dtype=torch.long)\n",
    "y_test = torch.zeros(num_samples, dtype=torch.long)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    X[i] = torch.randperm(seq_length)\n",
    "    y[i] = permutation_parity(X[i].tolist())\n",
    "    X_test[i] = torch.randperm(seq_length)\n",
    "    y_test[i] = permutation_parity(X_test[i].tolist())\n",
    "\n",
    "\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "ff_dim = 128\n",
    "\n",
    "model = TransformerModel(vocab_size, embed_dim, num_heads, num_layers, ff_dim, output_dim=2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4, weight_decay=1e-2)\n",
    "\n",
    "epochs = 2000\n",
    "batch_size = 64\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0.0\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        X_batch = X[i:i+batch_size]\n",
    "        y_batch = y[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_predictions = model(X_test)\n",
    "            test_loss = criterion(test_predictions, y_test)\n",
    "            _, predicted_classes = torch.max(test_predictions, 1)\n",
    "            accuracy = (predicted_classes == y_test).float().mean().item()\n",
    "        model.train()\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Avg Train Loss: {avg_loss/(num_samples//batch_size):.4f}, Test Loss: {test_loss.item():.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Avg Train Loss: 0.7073, Test Loss: 0.6956, Test Accuracy: 0.4974\n",
      "Epoch [21/2000], Avg Train Loss: 0.7020, Test Loss: 0.6932, Test Accuracy: 0.5026\n",
      "Epoch [41/2000], Avg Train Loss: 0.7019, Test Loss: 0.6932, Test Accuracy: 0.5026\n",
      "Epoch [61/2000], Avg Train Loss: 0.7019, Test Loss: 0.6932, Test Accuracy: 0.5026\n",
      "Epoch [81/2000], Avg Train Loss: 0.7019, Test Loss: 0.6932, Test Accuracy: 0.5026\n",
      "Epoch [101/2000], Avg Train Loss: 0.7019, Test Loss: 0.6932, Test Accuracy: 0.5026\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis Test\n",
    "Real-world application of the Transformer model for sentiment analysis. Note: not a gret test for positional encoding, but a good sanity check for the overall model.\n",
    "\n",
    "## Install required libraries\n",
    "`pip install datasets transformers`"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T10:50:28.100132Z",
     "start_time": "2026-01-09T10:50:25.081838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Load a small subset of the SST-2 dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"train[:5000]\")\n",
    "\n",
    "# 2. Tokenization (Turning words into numbers)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=16)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
    "\n",
    "# 3. Create DataLoader\n",
    "train_loader = DataLoader(tokenized_dataset, batch_size=32, shuffle=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd056549e56e468f9d50ac58bba41e87"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T10:50:33.656705Z",
     "start_time": "2026-01-09T10:50:33.636565Z"
    }
   },
   "source": [
    "# Updated Hyperparameters for Sentiment Analysis\n",
    "vocab_size = tokenizer.vocab_size  # Usually ~30,522 for BERT\n",
    "embed_dim = 32                     # Small embedding for speed\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "ff_dim = 128\n",
    "output_dim = 2                     # 0 for Negative, 1 for Positive\n",
    "\n",
    "# Initialize the model with the new vocab size\n",
    "# Note: You'll need to update your TransformerModel class to include an nn.Embedding layer\n",
    "model = TransformerModel(vocab_size, embed_dim, num_heads, num_layers, ff_dim, output_dim)\n",
    "\n",
    "# Use CrossEntropyLoss for classification instead of MSELoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T10:51:46.619264Z",
     "start_time": "2026-01-09T10:50:53.633131Z"
    }
   },
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        X = batch['input_ids']  # Shape: (batch_size, seq_length)\n",
    "        y = batch['label']      # Shape: (batch_size,)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(X)\n",
    "        loss = criterion(predictions, y)\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Avg Loss: {avg_loss/len(train_loader):.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Avg Loss: 0.5155\n",
      "Epoch [20/100], Avg Loss: 0.4026\n",
      "Epoch [30/100], Avg Loss: 0.3136\n",
      "Epoch [40/100], Avg Loss: 0.2346\n",
      "Epoch [50/100], Avg Loss: 0.1626\n",
      "Epoch [60/100], Avg Loss: 0.1054\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[41]\u001B[39m\u001B[32m, line 17\u001B[39m\n\u001B[32m     15\u001B[39m     \u001B[38;5;66;03m# Backward pass and optimization\u001B[39;00m\n\u001B[32m     16\u001B[39m     optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m     \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     18\u001B[39m     optimizer.step()\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (epoch + \u001B[32m1\u001B[39m) % \u001B[32m10\u001B[39m == \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/.torch_venv/lib/python3.11/site-packages/torch/_tensor.py:625\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    616\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    617\u001B[39m         Tensor.backward,\n\u001B[32m    618\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    623\u001B[39m         inputs=inputs,\n\u001B[32m    624\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m625\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/.torch_venv/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/.torch_venv/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    839\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    840\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    843\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    844\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    845\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T10:44:31.949827Z",
     "start_time": "2026-01-09T10:44:31.935805Z"
    }
   },
   "source": [
    "# 1. Put model in evaluation mode (disables dropout/batchnorm)\n",
    "model.eval()\n",
    "\n",
    "test_sequences = [\n",
    "    \"I absolutely loved this movie!\",\n",
    "    \"This was the worst film I have ever seen.\"\n",
    "]\n",
    "\n",
    "# 2. Tokenize (ensure return_tensors=\"pt\" for PyTorch)\n",
    "tokenized_test = tokenizer(test_sequences, padding=\"max_length\", truncation=True, max_length=16, return_tensors=\"pt\")\n",
    "X_test = tokenized_test['input_ids']  # Shape: (2, seq_length)\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test)\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    predictions = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "print(f\"Raw Logits: {logits.tolist()}\")\n",
    "print(f\"Probabilities: {probabilities.tolist()}\")\n",
    "print(f\"Predicted Classes (0 negative, 1 positive): {predictions.tolist()}\") # [1, 0] (hopefully!)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Logits: [[-0.7878147959709167, 1.1871830224990845], [1.6912055015563965, -1.2829406261444092]]\n",
      "Probabilities: [[0.12185309082269669, 0.8781468868255615], [0.9513923525810242, 0.04860762506723404]]\n",
      "Predicted Classes (0 negative, 1 positive): [1, 0]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
